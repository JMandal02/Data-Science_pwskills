{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNfn99F1i59vzTM4fshGnht",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JMandal02/Data-Science_pwskills/blob/main/Assignment_Ensemble_Techniques.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 1: What is Ensemble Learning in machine learning? Explain the key idea behind it.**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "Ensemble Learning is a machine learning technique where multiple individual models (called base learners or weak learners) are combined to form a stronger predictive model.  \n",
        "The main idea is that a group of weak models can work together to produce better performance and generalization than any single model alone.\n",
        "\n",
        "In other words, instead of relying on one model’s predictions, ensemble learning aggregates the predictions from several models (through averaging, voting, or weighting) to reduce variance, bias, or improve accuracy.\n",
        "\n",
        "**Key Idea:**\n",
        "- Each individual model may make some errors, but their combination helps cancel out individual mistakes.\n",
        "- It leverages diversity among models — models should make different types of errors.\n",
        "- Common ensemble techniques include **Bagging, Boosting, and Stacking**.\n",
        "\n",
        "**Benefits:**\n",
        "- Improves prediction accuracy.\n",
        "- Reduces overfitting.\n",
        "- Increases model robustness and generalization.\n",
        "\n",
        "---\n",
        "\n",
        "# **Question 2: What is the difference between Bagging and Boosting?**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "Both Bagging and Boosting are ensemble learning methods, but they differ in how they train and combine models.\n",
        "\n",
        "| Feature | **Bagging (Bootstrap Aggregating)** | **Boosting** |\n",
        "|----------|------------------------------------|---------------|\n",
        "| **Goal** | Reduce variance | Reduce bias |\n",
        "| **Training** | Models are trained **independently** in parallel | Models are trained **sequentially**, where each model focuses on correcting errors of the previous one |\n",
        "| **Data Sampling** | Uses random sampling **with replacement** (bootstrap samples) | Each new model is trained on data weighted by the previous model’s errors |\n",
        "| **Combination** | Aggregates results by majority vote (classification) or averaging (regression) | Combines results using weighted voting or summation based on model performance |\n",
        "| **Example Algorithms** | Random Forest | AdaBoost, Gradient Boosting, XGBoost |\n",
        "| **Overfitting** | Less prone to overfitting | Can overfit if too many weak learners are added |\n",
        "\n",
        "**In short:**  \n",
        "- Bagging reduces variance by averaging over multiple models.  \n",
        "- Boosting reduces bias by focusing on difficult examples and sequentially improving weak learners.\n",
        "\n",
        "---\n",
        "\n",
        "# **Question 3: What is bootstrap sampling and what role does it play in Bagging methods like Random Forest?**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Bootstrap Sampling** is a statistical method that involves sampling data points **with replacement** from the original dataset to create multiple subsets (bootstrap samples) of the same size as the original data.\n",
        "\n",
        "Each subset may contain duplicate instances because sampling is done with replacement.\n",
        "\n",
        "**Role in Bagging:**\n",
        "- In Bagging (like Random Forest), each model is trained on a different bootstrap sample.\n",
        "- This creates diversity among models since each one sees a slightly different version of the data.\n",
        "- When predictions from all models are combined (through averaging or voting), the overall model variance is reduced.\n",
        "\n",
        "**In Random Forest:**\n",
        "- Each decision tree is trained on a unique bootstrap sample.\n",
        "- This randomness helps ensure that trees are decorrelated, which leads to better ensemble performance.\n",
        "\n",
        "**Example:**  \n",
        "If the dataset has 1000 records, each tree in the Random Forest might train on a random sample of 1000 records (with replacement).\n",
        "\n",
        "---\n",
        "\n",
        "# **Question 4: What are Out-of-Bag (OOB) samples and how is OOB score used to evaluate ensemble models?**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Out-of-Bag (OOB) samples** are the data points **not included** in a particular bootstrap sample during Bagging.\n",
        "\n",
        "Since each bootstrap sample is created with replacement, roughly **36.8% of the data** remains unused for each model — these are the OOB samples.\n",
        "\n",
        "**OOB Score:**\n",
        "- OOB samples act like a validation set for each model.\n",
        "- The model is tested on its OOB samples, and performance is averaged across all models.\n",
        "- This gives an unbiased estimate of model accuracy without needing a separate validation set.\n",
        "\n",
        "**Advantages:**\n",
        "- Efficient — no need to reserve additional validation data.\n",
        "- Provides an internal estimate of generalization error.\n",
        "\n",
        "**In Random Forest:**\n",
        "```python\n",
        "RandomForestClassifier(oob_score=True)\n"
      ],
      "metadata": {
        "id": "bqXVnf2pjLzF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 5: Compare feature importance analysis in a single Decision Tree vs. a Random Forest.**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Feature Importance** measures how much each feature contributes to a model’s predictions.  \n",
        "It helps us understand which variables are most influential in decision-making.\n",
        "\n",
        "---\n",
        "\n",
        "### **Comparison Table**\n",
        "\n",
        "| Aspect | **Decision Tree** | **Random Forest** |\n",
        "|--------|-------------------|-------------------|\n",
        "| **Computation Method** | Based on reduction in impurity (Gini or Entropy) from each split within the single tree. | Computed as the *average* of feature importance scores across all trees in the forest. |\n",
        "| **Bias** | Can be biased toward features with many categories or continuous values. | Reduces bias through model averaging, giving a more balanced view. |\n",
        "| **Stability** | Sensitive to small data changes — importance values may fluctuate. | More stable because results are aggregated from multiple trees. |\n",
        "| **Interpretability** | Easy to interpret due to single model structure. | Harder to visualize, but provides more reliable importance values. |\n",
        "| **Performance** | May overfit and rely too heavily on dominant features. | Handles noise and irrelevant features better due to ensemble averaging. |\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "2tw9LblfjQi8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 6:**\n",
        "### Write a Python program to:\n",
        "- Load the Breast Cancer dataset using `sklearn.datasets.load_breast_cancer()`\n",
        "- Train a Random Forest Classifier\n",
        "- Print the top 5 most important features based on feature importance scores.\n"
      ],
      "metadata": {
        "id": "GNnJGVJDlgkw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "# Initialize and train Random Forest model\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X, y)\n",
        "\n",
        "# Get feature importances\n",
        "importances = rf.feature_importances_\n",
        "\n",
        "# Create a DataFrame for better readability\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': data.feature_names,\n",
        "    'Importance': importances\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Print the top 5 important features\n",
        "print(\"Top 5 Most Important Features:\")\n",
        "print(feature_importance_df.head(5))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwdVpLEwliQA",
        "outputId": "e6d9deae-39bd-4056-872f-720938b5f49d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Most Important Features:\n",
            "                 Feature  Importance\n",
            "23            worst area    0.139357\n",
            "27  worst concave points    0.132225\n",
            "7    mean concave points    0.107046\n",
            "20          worst radius    0.082848\n",
            "22       worst perimeter    0.080850\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 7:**\n",
        "### Write a Python program to:\n",
        "- Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
        "- Evaluate its accuracy and compare with a single Decision Tree\n"
      ],
      "metadata": {
        "id": "GaBHSni7mRBi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    iris.data, iris.target, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Single Decision Tree\n",
        "tree = DecisionTreeClassifier(random_state=42)\n",
        "tree.fit(X_train, y_train)\n",
        "y_pred_tree = tree.predict(X_test)\n",
        "tree_acc = accuracy_score(y_test, y_pred_tree)\n",
        "\n",
        "# Bagging Classifier with Decision Tree as base estimator\n",
        "bagging = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "bagging.fit(X_train, y_train)\n",
        "y_pred_bag = bagging.predict(X_test)\n",
        "bag_acc = accuracy_score(y_test, y_pred_bag)\n",
        "\n",
        "print(\"Accuracy of Single Decision Tree:\", tree_acc)\n",
        "print(\"Accuracy of Bagging Classifier:\", bag_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "up5aFbismQcy",
        "outputId": "2ce0aa24-45d6-4191-e0d6-db83f31f0021"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Single Decision Tree: 1.0\n",
            "Accuracy of Bagging Classifier: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 8:**\n",
        "### Write a Python program to:\n",
        "- Train a Random Forest Classifier\n",
        "- Tune hyperparameters `max_depth` and `n_estimators` using `GridSearchCV`\n",
        "- Print the best parameters and final accuracy"
      ],
      "metadata": {
        "id": "CXd_Gr8cmrAa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    iris.data, iris.target, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Define Random Forest model\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Define parameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'max_depth': [3, 5, 7, None]\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=rf,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Fit GridSearchCV\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get best model and evaluate\n",
        "best_rf = grid_search.best_estimator_\n",
        "y_pred = best_rf.predict(X_test)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Final Accuracy:\", acc)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4zkR0BFmnHz",
        "outputId": "1561d709-1e9a-4ed5-85dd-b5f245fd64d1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 3, 'n_estimators': 150}\n",
            "Final Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 9:**\n",
        "### Write a Python program to:\n",
        "- Train a Bagging Regressor and a Random Forest Regressor on the California Housing dataset\n",
        "- Compare their Mean Squared Errors (MSE)\n"
      ],
      "metadata": {
        "id": "_capH226m9us"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "housing = fetch_california_housing()\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    housing.data, housing.target, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Bagging Regressor\n",
        "bag_reg = BaggingRegressor(\n",
        "    estimator=DecisionTreeRegressor(),\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "bag_reg.fit(X_train, y_train)\n",
        "y_pred_bag = bag_reg.predict(X_test)\n",
        "mse_bag = mean_squared_error(y_test, y_pred_bag)\n",
        "\n",
        "# Random Forest Regressor\n",
        "rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_reg.fit(X_train, y_train)\n",
        "y_pred_rf = rf_reg.predict(X_test)\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "\n",
        "print(\"Mean Squared Error (Bagging Regressor):\", mse_bag)\n",
        "print(\"Mean Squared Error (Random Forest Regressor):\", mse_rf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqbYJWrrnDMi",
        "outputId": "61692369-547e-4eba-9206-02c9a6b69ccd"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (Bagging Regressor): 0.25787382250585034\n",
            "Mean Squared Error (Random Forest Regressor): 0.25650512920799395\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 10: Real-world Ensemble Learning – Loan Default Prediction**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Scenario:**  \n",
        "You are a data scientist at a financial institution tasked with predicting **loan default** using customer demographics and transaction data.  \n",
        "You plan to use **Ensemble Learning** to improve prediction accuracy and decision reliability.\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 1: Choose between Bagging or Boosting**\n",
        "\n",
        "| Case | Recommended Method | Reason |\n",
        "|------|--------------------|--------|\n",
        "| High variance (model overfits easily) | **Bagging (e.g., Random Forest)** | Reduces variance via averaging multiple independent models. |\n",
        "| High bias (model underfits or misses patterns) | **Boosting (e.g., AdaBoost, XGBoost, Gradient Boosting)** | Sequentially focuses on difficult samples and reduces bias. |\n",
        "\n",
        "In loan default prediction, **Boosting** is often preferred because it handles **imbalanced and complex financial data** effectively.\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 2: Handle Overfitting**\n",
        "\n",
        "- Use **cross-validation** for tuning parameters.  \n",
        "- Apply **early stopping** (in Boosting).  \n",
        "- Use **regularization parameters** (L1/L2 penalties).  \n",
        "- Limit **max_depth** and **n_estimators** to prevent overly complex models.  \n",
        "- Drop highly correlated or irrelevant features.\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 3: Select Base Models**\n",
        "\n",
        "- For **Bagging** → Decision Tree as base estimator (Random Forest).  \n",
        "- For **Boosting** → Shallow trees or weak learners as base models.  \n",
        "- Optionally test Logistic Regression or SVM as base models for diversity.\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 4: Evaluate Performance using Cross-Validation**\n",
        "\n",
        "- Use **K-Fold Cross-Validation** (e.g., k = 5).  \n",
        "- Evaluate with metrics suited for classification:  \n",
        "  - Accuracy  \n",
        "  - Precision & Recall  \n",
        "  - F1-Score  \n",
        "  - ROC-AUC (useful for imbalanced datasets)\n",
        "\n",
        "- Compare ensemble accuracy with that of a single model baseline.\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 5: Justify How Ensemble Learning Improves Decision-Making**\n",
        "\n",
        "- **Combines multiple weak models** to improve robustness.  \n",
        "- **Reduces overfitting** — predictions generalize better to unseen customers.  \n",
        "- **Improves recall and precision**, reducing false loan approvals or rejections.  \n",
        "- **Provides feature importance** — helps identify top risk factors like Credit Score, Debt-to-Income Ratio, and Payment History.  \n",
        "- Enables **data-driven and explainable decisions** for credit risk management.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "VAjNmeVHkgD4"
      }
    }
  ]
}