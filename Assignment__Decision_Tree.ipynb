{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNTFw+g7R1t5QwQB9zqbTzs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JMandal02/Data-Science_pwskills/blob/main/Assignment__Decision_Tree.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# **Assignment -- Decision Tree**"
      ],
      "metadata": {
        "id": "qUW6A52wTeIX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q1: What is a Decision Tree, and how does it work in the context of classification?**\n",
        "\n",
        "A **Decision Tree** is a supervised machine learning model that predicts an output by learning **if-else decision rules** from the data.\n",
        "\n",
        "- It splits the dataset based on **feature values** to create branches.\n",
        "- Each **internal node** represents a decision (e.g., “Petal length ≤ 2.5?”)\n",
        "- Each **leaf node** represents a final prediction (e.g., Iris Setosa)\n",
        "\n",
        "### **In Classification:**\n",
        "The goal is to **separate different classes** step-by-step by choosing the **best splitting condition** at each node.\n",
        "\n",
        "Decision Tree tries to make **each split as pure as possible** — meaning most records in a node belong to one class only.\n",
        "\n",
        "---\n",
        "\n",
        "# **Q2: Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact splits in a Decision Tree?**\n",
        "\n",
        "These are used to measure **how mixed the classes are** in a node.\n",
        "\n",
        "### **Gini Impurity (used in CART / sklearn default)**\n",
        "Gini = 1 − Σ pₙ²  \n",
        "- 0 = pure node (only one class)  \n",
        "- Higher = more mixed  \n",
        "\n",
        "### **Entropy (used in ID3 / Information Gain)**\n",
        "Entropy = − Σ pₙ log₂(pₙ)  \n",
        "- 0 = pure  \n",
        "- High value = high disorder  \n",
        "\n",
        "### **Impact on splits:**\n",
        "- Decision Tree chooses the **split that reduces impurity the most**\n",
        "- Both give similar results, but **Gini is slightly faster**\n",
        "- Entropy is more **mathematically precise**\n",
        "\n",
        "---\n",
        "\n",
        "# **Q3: What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of each.**\n",
        "\n",
        "|Type | Description | Advantage |\n",
        "|------|-------------|-----------|\n",
        "| **Pre-Pruning** | Stop growing the tree early using constraints like `max_depth`, `min_samples_split` | **Prevents overfitting + Faster training** |\n",
        "| **Post-Pruning** | First grow full tree, then remove unnecessary branches later | **Better accuracy (removes only harmful branches)** |\n",
        "\n",
        "---\n",
        "\n",
        "# **Q4: What is Information Gain in Decision Trees, and why is it important for choosing the best split?**\n",
        "\n",
        "**Information Gain = Reduction in Entropy after a split**  \n",
        "IG = Entropy(parent) − Weighted Entropy(children)\n",
        "\n",
        "- It measures **how much uncertainty is removed** after a split.\n",
        "- The **split with the highest Information Gain** is selected.\n",
        "- Ensures the tree splits on the **most informative feature first**\n",
        "\n",
        "---\n",
        "\n",
        "# **Q5: What are some common real-world applications of Decision Trees? What are their advantages and limitations?**\n",
        "\n",
        "### **Real-world Applications:**\n",
        "- Medical diagnosis  \n",
        "- Credit loan approval  \n",
        "- Fraud detection  \n",
        "- Customer churn prediction  \n",
        "- Spam filtering  \n",
        "\n",
        "### **Advantages:**\n",
        "- **Easy to understand & interpret**\n",
        "- **No need for feature scaling**\n",
        "- Works with **numerical + categorical data**\n",
        "\n",
        "### **Limitations:**\n",
        "- **Can overfit easily** if not pruned\n",
        "- **Unstable** — small data change → different tree\n",
        "- **Less accurate than Random Forest / XGBoost**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "v45In98rTUdx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 6: Write a Python program to:**\n",
        "\n",
        "### **● Load the Iris Dataset**\n",
        "\n",
        "### **● Train a Decision Tree Classifier using the Gini criterion**\n",
        "\n",
        "### **● Print the model’s accuracy and feature importances**"
      ],
      "metadata": {
        "id": "tFwwP7grqr9h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab-ready cell for Q6\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load data\n",
        "iris = load_iris(as_frame=True)\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "feature_names = iris.feature_names\n",
        "class_names = iris.target_names\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
        "\n",
        "# Train Decision Tree with Gini criterion (default)\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = clf.predict(X_test)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Feature importances\n",
        "importances = clf.feature_importances_\n",
        "feat_imp = pd.Series(importances, index=feature_names).sort_values(ascending=False)\n",
        "\n",
        "print(f\"Accuracy (test): {acc:.4f}\")\n",
        "print(\"\\nFeature importances:\")\n",
        "print(feat_imp)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fzIdm2otQmB",
        "outputId": "d12694fd-cab2-4b33-e78e-f058858df267"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (test): 0.8947\n",
            "\n",
            "Feature importances:\n",
            "petal length (cm)    0.919887\n",
            "petal width (cm)     0.046629\n",
            "sepal width (cm)     0.020091\n",
            "sepal length (cm)    0.013394\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 7: Write a Python program to:**\n",
        "\n",
        "### **● Load the Iris Dataset**\n",
        "\n",
        "### **● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to a fully-grown tree.**"
      ],
      "metadata": {
        "id": "i78Bp5ZSsQtN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab-ready cell for Q7\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris(as_frame=True)\n",
        "X = iris.data; y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
        "\n",
        "# Fully grown tree (no depth limit)\n",
        "clf_full = DecisionTreeClassifier(random_state=42)\n",
        "clf_full.fit(X_train, y_train)\n",
        "y_pred_full = clf_full.predict(X_test)\n",
        "acc_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "# Shallow tree with max_depth=3\n",
        "clf_shallow = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "clf_shallow.fit(X_train, y_train)\n",
        "y_pred_shallow = clf_shallow.predict(X_test)\n",
        "acc_shallow = accuracy_score(y_test, y_pred_shallow)\n",
        "\n",
        "print(f\"Fully grown tree accuracy: {acc_full:.4f}\")\n",
        "print(f\"max_depth=3 tree accuracy:  {acc_shallow:.4f}\")\n",
        "\n",
        "# Optional: print depth and number of leaves for comparison\n",
        "print(\"\\nFully grown tree depth:\", clf_full.get_depth(), \" leaves:\", clf_full.get_n_leaves())\n",
        "print(\"max_depth=3 tree depth:\", clf_shallow.get_depth(), \" leaves:\", clf_shallow.get_n_leaves())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZNITu97tRKW",
        "outputId": "dc34f62a-65fa-4da4-e628-bd5651343488"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fully grown tree accuracy: 0.8947\n",
            "max_depth=3 tree accuracy:  0.8947\n",
            "\n",
            "Fully grown tree depth: 6  leaves: 9\n",
            "max_depth=3 tree depth: 3  leaves: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 8: Write a Python program to:**\n",
        "\n",
        "### **● Load the Boston Housing Dataset**\n",
        "\n",
        "### **● Train a Decision Tree Regressor**\n",
        "\n",
        "### **● Print the Mean Squared Error (MSE) and feature importances**\n"
      ],
      "metadata": {
        "id": "9CiPLdLUqr6d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab-ready cell for Q8\n",
        "# Note: load_boston() has been deprecated/removed in recent sklearn.\n",
        "# We'll fetch the Boston dataset from OpenML (data_id=531). If fetch_openml is not allowed,\n",
        "# an alternative is fetch_california_housing() or loading a CSV from a known source.\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import pandas as pd\n",
        "\n",
        "# Fetch Boston from OpenML (ID 531)\n",
        "boston = fetch_openml(data_id=531, as_frame=True)  # ID 531 corresponds to \"boston\" on OpenML\n",
        "X = boston.data\n",
        "y = boston.target.astype(float)  # target might be a string; convert to float\n",
        "feature_names = X.columns.tolist()\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "# Train Decision Tree Regressor (default settings)\n",
        "reg = DecisionTreeRegressor(random_state=42)\n",
        "reg.fit(X_train, y_train)\n",
        "\n",
        "# Predict & evaluate\n",
        "y_pred = reg.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "# Feature importances\n",
        "importances = pd.Series(reg.feature_importances_, index=feature_names).sort_values(ascending=False)\n",
        "\n",
        "print(f\"Mean Squared Error (test): {mse:.4f}\")\n",
        "print(\"\\nFeature importances:\")\n",
        "print(importances)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FajrSaaitRt8",
        "outputId": "c64892f8-f7c1-4019-a2e7-f78af295e052"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (test): 16.6884\n",
            "\n",
            "Feature importances:\n",
            "RM         0.587170\n",
            "LSTAT      0.210344\n",
            "DIS        0.073912\n",
            "CRIM       0.066320\n",
            "AGE        0.014041\n",
            "INDUS      0.011459\n",
            "B          0.011301\n",
            "PTRATIO    0.009587\n",
            "NOX        0.007032\n",
            "TAX        0.005635\n",
            "ZN         0.001315\n",
            "CHAS       0.001127\n",
            "RAD        0.000758\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 9: Write a Python program to:**\n",
        "\n",
        "### **● Load the Iris Dataset**\n",
        "\n",
        "### **● Tune the Decision Tree’s max_depth and min_samples_split using GridSearchCV**\n",
        "\n",
        "### **● Print the best parameters and the resulting model accuracy**\n"
      ],
      "metadata": {
        "id": "qdQ0eQUsqr3X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab-ready cell for Q9\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris(as_frame=True)\n",
        "X = iris.data; y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
        "\n",
        "param_grid = {\n",
        "    'max_depth': [None, 1, 2, 3, 4, 5],\n",
        "    'min_samples_split': [2, 4, 6, 8, 10]\n",
        "}\n",
        "\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "grid = GridSearchCV(dt, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "best_params = grid.best_params_\n",
        "best_score_cv = grid.best_score_\n",
        "\n",
        "# Evaluate best estimator on test set\n",
        "best_est = grid.best_estimator_\n",
        "test_pred = best_est.predict(X_test)\n",
        "test_acc = accuracy_score(y_test, test_pred)\n",
        "\n",
        "print(\"Best parameters (GridSearchCV):\", best_params)\n",
        "print(f\"Best CV accuracy (train-folds): {best_score_cv:.4f}\")\n",
        "print(f\"Test accuracy with best params:     {test_acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0oBkWd_tSLu",
        "outputId": "72eaeaee-d3f0-42e6-b2bb-3a8ecea412b8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters (GridSearchCV): {'max_depth': None, 'min_samples_split': 4}\n",
            "Best CV accuracy (train-folds): 0.9375\n",
            "Test accuracy with best params:     0.8947\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Q10: Imagine you’re working as a data scientist for a healthcare company that**\n",
        "### **wants to predict whether a patient has a certain disease. You have a large dataset with mixed data types and some missing values.**\n",
        "### Explain the step-by-step process you would follow to:\n",
        "\n",
        "### **● Handle the missing values**\n",
        "\n",
        "### **● Encode the categorical features**\n",
        "\n",
        "### **● Train a Decision Tree model**\n",
        "\n",
        "### **● Tune its hyperparameters**\n",
        "\n",
        "### **● Evaluate its performance**\n",
        "\n",
        "### **And describe what business value this model could provide in the real-world setting.**\n",
        "\n",
        "### **Answer**\n",
        "\n",
        "### **Step 1: Handle Missing Values**\n",
        "- Numerical: **Median Imputation**\n",
        "- Categorical: **“Missing” category or most frequent value**\n",
        "- Optionally add **“was_missing” flag** for medical reasoning\n",
        "\n",
        "### **Step 2: Encode Categorical Features**\n",
        "- Use **One-Hot Encoding** (safe + interpretable)\n",
        "- If too many categories → use **Target Encoding**\n",
        "\n",
        "### **Step 3: Train Decision Tree Model**\n",
        "- Use sklearn `Pipeline` + `ColumnTransformer`\n",
        "- Control complexity with `max_depth`, `min_samples_split`\n",
        "\n",
        "### **Step 4: Hyperparameter Tuning**\n",
        "- Use `GridSearchCV` or `RandomizedSearchCV`\n",
        "- Important params: `max_depth`, `min_samples_leaf`, `ccp_alpha`\n",
        "\n",
        "### **Step 5: Evaluate Performance**\n",
        "- Metrics: **Accuracy, Precision, Recall, F1, ROC-AUC**\n",
        "- **Healthcare Priority = Minimize False Negatives**\n",
        "- Show **Confusion Matrix** to medical experts\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "_gw5xNErTkaI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVIay0x_TS4M"
      },
      "outputs": [],
      "source": []
    }
  ]
}